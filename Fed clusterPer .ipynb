{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "import copy\n",
    "from tqdm import tqdm  # For tracking training progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example list of labels\n",
    "label_names = ['cat', 'dog', 'bird', 'fish', 'car', 'aircraft', 'flower', 'truck', 'parachute', 'mushroom']\n",
    "\n",
    "# Create a mapping from label names to indices\n",
    "label_to_index = {label: idx for idx, label in enumerate(label_names)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3), # Convert grayscale to 3 channels (RGB)\n",
    "    transforms.Resize((256, 256)),  # Resize all images to 256x256\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def apply_transform(example):\n",
    "    # Check if 'example['image']' is a list (batch of images)\n",
    "    transformed_images = [transform(img) for img in example['image']]\n",
    "    labels = [label_to_index[label] for label in example['label']]\n",
    "    # Return the transformed images and the unchanged labels\n",
    "    return {\n",
    "        'image': transformed_images,  # Stack to create a single tensor\n",
    "        'label': torch.tensor(labels)  # Convert labels to tensor\n",
    "    }\n",
    "\n",
    "# Apply the transformations to the dataset (train + test split for each \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_custom_dataloader(dataset, batch_size=16):\n",
    "    # Apply the transformation to each sample in the dataset\n",
    "    dataset = dataset.with_transform(apply_transform)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(dataset['train'], batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset['test'], batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = load_dataset(\"AnnantJain/client1_federated_dataset_modified\")\n",
    "dataset_2 = load_dataset(\"AnnantJain/client2_federated_dataset_modified\")\n",
    "dataset_3 = load_dataset(\"AnnantJain/client3_federated_dataset_modified\")\n",
    "dataset_4 = load_dataset(\"AnnantJain/client4_federated_dataset_modified\")\n",
    "dataset_5 = load_dataset(\"AnnantJain/client5_federated_dataset_modified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_1, test_loader_1 = prepare_custom_dataloader(dataset_1)\n",
    "train_loader_2, test_loader_2 = prepare_custom_dataloader(dataset_2)\n",
    "train_loader_3, test_loader_3 = prepare_custom_dataloader(dataset_3)\n",
    "train_loader_4, test_loader_4 = prepare_custom_dataloader(dataset_4)\n",
    "train_loader_5, test_loader_5 = prepare_custom_dataloader(dataset_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        # Define layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(32 * 64 * 64, 128)  # Adjust based on output size from conv layers\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.ReLU()(self.conv1(x)))  # Conv Layer 1\n",
    "        x = self.pool((nn.ReLU()(self.conv2(x))))  # Conv Layer 2\n",
    "        x = x.view(-1, 32 * 64 * 64)  # Flatten for fully connected layer\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_local(model, train_loader, criterion, optimizer, epochs=2):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train_loader:\n",
    "            images = batch['image']  # This should be a tensor\n",
    "            labels = batch['label'] \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            images = batch['image']  # This should be a tensor\n",
    "            labels = batch['label'] \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Function to average the weights of global layers (FedAvg)\n",
    "def average_global_weights(global_model, client_models):\n",
    "    global_state_dict = global_model.state_dict()\n",
    "    for key in global_state_dict.keys():\n",
    "        # Average the global layers (shared part)\n",
    "        global_state_dict[key] = torch.mean(\n",
    "            torch.stack([client_models[i].state_dict()[key] for i in range(len(client_models))]), dim=0\n",
    "        )\n",
    "    global_model.load_state_dict(global_state_dict)\n",
    "\n",
    "\n",
    "\n",
    "def average_global_weights1(global_model, client_models, client_weights):\n",
    "    global_state_dict = global_model.state_dict()\n",
    "    for key in global_state_dict.keys():\n",
    "        weighted_sum = torch.zeros_like(global_state_dict[key])\n",
    "        total_weight = 0.0\n",
    "\n",
    "        # Weighted sum of the model weights from clients based on their performance\n",
    "        for i, client_model in enumerate(client_models):\n",
    "            client_weight = client_weights[i]\n",
    "            weighted_sum += client_weight * client_model.state_dict()[key]\n",
    "            total_weight += client_weight\n",
    "\n",
    "        global_state_dict[key] = weighted_sum / total_weight\n",
    "\n",
    "    global_model.load_state_dict(global_state_dict)\n",
    "\n",
    "\n",
    "# Average pruned weights across all clients for selective layers\n",
    "def selective_average_global_weights(global_model, client_models, client_weights):\n",
    "    global_state_dict = global_model.state_dict()\n",
    "    for key in global_state_dict.keys():\n",
    "        weighted_sum = torch.zeros_like(global_state_dict[key])\n",
    "        total_weight = 0.0\n",
    "        for i, client_model in enumerate(client_models):\n",
    "            client_weight = client_weights[i]\n",
    "            weighted_sum += client_weight * client_model.state_dict().get(key, 0)\n",
    "            total_weight += client_weight\n",
    "        global_state_dict[key] = weighted_sum / total_weight\n",
    "    global_model.load_state_dict(global_state_dict, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Prune parameters by retaining only the top fraction of important weights\n",
    "def prune_model_weights(model, prune_fraction=0.3):\n",
    "    pruned_state_dict = {}\n",
    "    for name, param in model.state_dict().items():\n",
    "        threshold = torch.quantile(param.abs(), prune_fraction)\n",
    "        pruned_param = param * (param.abs() > threshold)  # Zero out less important weights\n",
    "        pruned_state_dict[name] = pruned_param\n",
    "    return pruned_state_dict\n",
    "\n",
    "def distill_logits(logits, targets, temperature=2.0):\n",
    "    return nn.functional.softmax(logits / temperature, dim=1)\n",
    "\n",
    "# 1. Freeze selective layers after a few rounds\n",
    "def freeze_layers(model, layers_to_freeze=['conv1', 'conv2']):\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(layer in name for layer in layers_to_freeze):\n",
    "            param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine the cluster based on cosine similarity\n",
    "def determine_cluster(client_model, cluster_models):\n",
    "    max_similarity = -1\n",
    "    best_cluster_id = 0\n",
    "    client_state_dict = client_model.state_dict()\n",
    "    \n",
    "    for cluster_id, cluster_model in enumerate(cluster_models):\n",
    "        cluster_state_dict = cluster_model.state_dict()\n",
    "        similarity_sum = 0\n",
    "        num_layers = 0\n",
    "        \n",
    "        for layer_name in client_state_dict:\n",
    "            if layer_name in cluster_state_dict:\n",
    "                client_layer = client_state_dict[layer_name].flatten()\n",
    "                cluster_layer = cluster_state_dict[layer_name].flatten()\n",
    "                \n",
    "                similarity = cosine_similarity(client_layer.unsqueeze(0), cluster_layer.unsqueeze(0), dim=1)\n",
    "                similarity_sum += similarity.item()\n",
    "                num_layers += 1\n",
    "\n",
    "        avg_similarity = similarity_sum / num_layers if num_layers > 0 else 0\n",
    "\n",
    "        if avg_similarity > max_similarity:\n",
    "            max_similarity = avg_similarity\n",
    "            best_cluster_id = cluster_id\n",
    "\n",
    "    return best_cluster_id\n",
    "\n",
    "\n",
    "def average_cluster_weights(global_model, cluster_models, cluster_weights):\n",
    "    global_state_dict = global_model.state_dict()\n",
    "    for key in global_state_dict.keys():\n",
    "        weighted_sum = torch.zeros_like(global_state_dict[key])\n",
    "        total_weight = 0.0\n",
    "        for i, client_model in enumerate(cluster_models):\n",
    "            client_weight = cluster_weights[i]\n",
    "            weighted_sum += client_weight * client_model.state_dict().get(key, 0)\n",
    "            total_weight += client_weight\n",
    "        global_state_dict[key] = weighted_sum / total_weight\n",
    "    global_model.load_state_dict(global_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = [\n",
    "    (train_loader_1, test_loader_1),\n",
    "    (train_loader_2, test_loader_2),\n",
    "    (train_loader_3, test_loader_3),\n",
    "    (train_loader_4, test_loader_4),\n",
    "    (train_loader_5, test_loader_5)\n",
    "]\n",
    "\n",
    "# Initialize the global model\n",
    "global_model = CNN(num_classes=10)\n",
    "num_clusters = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1/5\n",
      "Client 1 local training...\n",
      "Client 2 local training...\n",
      "Client 3 local training...\n",
      "Client 4 local training...\n",
      "Client 5 local training...\n",
      "Updating Cluster 1 model with 5 clients.\n",
      "Client 1 Accuracy: 77.04%\n",
      "Client 2 Accuracy: 72.89%\n",
      "Client 3 Accuracy: 55.83%\n",
      "Client 4 Accuracy: 55.33%\n",
      "Client 5 Accuracy: 64.38%\n",
      "Round 2/5\n",
      "Client 1 local training...\n",
      "Client 2 local training...\n",
      "Client 3 local training...\n",
      "Client 4 local training...\n",
      "Client 5 local training...\n",
      "Updating Cluster 1 model with 5 clients.\n",
      "Client 1 Accuracy: 80.37%\n",
      "Client 2 Accuracy: 71.33%\n",
      "Client 3 Accuracy: 49.44%\n",
      "Client 4 Accuracy: 50.00%\n",
      "Client 5 Accuracy: 57.71%\n",
      "Round 3/5\n",
      "Client 1 local training...\n",
      "Client 2 local training...\n",
      "Client 3 local training...\n",
      "Client 4 local training...\n",
      "Client 5 local training...\n",
      "Updating Cluster 1 model with 5 clients.\n",
      "Client 1 Accuracy: 81.85%\n",
      "Client 2 Accuracy: 76.67%\n",
      "Client 3 Accuracy: 54.44%\n",
      "Client 4 Accuracy: 55.67%\n",
      "Client 5 Accuracy: 65.00%\n",
      "Round 4/5\n",
      "Client 1 local training...\n",
      "Client 2 local training...\n",
      "Client 3 local training...\n",
      "Client 4 local training...\n",
      "Client 5 local training...\n",
      "Updating Cluster 1 model with 5 clients.\n",
      "Client 1 Accuracy: 78.52%\n",
      "Client 2 Accuracy: 64.00%\n",
      "Client 3 Accuracy: 45.28%\n",
      "Client 4 Accuracy: 33.00%\n",
      "Client 5 Accuracy: 43.33%\n",
      "Round 5/5\n",
      "Client 1 local training...\n",
      "Client 2 local training...\n",
      "Client 3 local training...\n",
      "Client 4 local training...\n",
      "Client 5 local training...\n",
      "Updating Cluster 1 model with 5 clients.\n",
      "Client 1 Accuracy: 80.00%\n",
      "Client 2 Accuracy: 63.11%\n",
      "Client 3 Accuracy: 41.94%\n",
      "Client 4 Accuracy: 38.00%\n",
      "Client 5 Accuracy: 43.12%\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "def federated_learning(clients, global_model, num_rounds=5, prune_fraction=0.3, freeze_after_round=3):\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    # Initialize clusters with copies of the global model\n",
    "    cluster_models = [copy.deepcopy(global_model) for _ in range(num_clusters)]\n",
    "    \n",
    "    for round in range(num_rounds):\n",
    "        print(f\"Round {round+1}/{num_rounds}\")\n",
    "\n",
    "        # Step 1: Train personalized models locally\n",
    "        local_models = []\n",
    "        client_weights = []\n",
    "        for i, (train_loader, test_loader) in enumerate(clients):\n",
    "            print(f\"Client {i+1} local training...\")\n",
    "            local_model = copy.deepcopy(global_model)  # Clone the global model\n",
    "            \n",
    "            optimizer = optim.Adam(local_model.parameters(), lr=0.001)\n",
    "\n",
    "            # Train on each client's data\n",
    "            train_local(local_model, train_loader, criterion, optimizer)\n",
    "            local_models.append(local_model)\n",
    "\n",
    "            acc = evaluate(local_model, test_loader)\n",
    "            client_weights.append(acc)\n",
    "\n",
    "        # Step 2: Average global layers across clients (FedAvg)\n",
    "        cluster_clients = [[] for _ in range(num_clusters)]\n",
    "        cluster_client_weights = [[] for _ in range(num_clusters)]\n",
    "\n",
    "        for i, local_model in enumerate(local_models):\n",
    "            cluster_id = determine_cluster(local_model, cluster_models)\n",
    "            cluster_clients[cluster_id].append(local_model)\n",
    "            cluster_client_weights[cluster_id].append(client_weights[i])\n",
    "\n",
    "        # Update each cluster model by averaging weights\n",
    "        for cluster_id in range(num_clusters):\n",
    "            if cluster_clients[cluster_id]:\n",
    "                print(f\"Updating Cluster {cluster_id+1} model with {len(cluster_clients[cluster_id])} clients.\")\n",
    "                average_cluster_weights(cluster_models[cluster_id], cluster_clients[cluster_id], cluster_client_weights[cluster_id])\n",
    "\n",
    "\n",
    "        # Step 3: Evaluate each personalized model after federated update\n",
    "        for i, (train_loader, test_loader) in enumerate(clients):\n",
    "            acc = evaluate(local_models[i], test_loader)\n",
    "            print(f\"Client {i+1} Accuracy: {acc * 100:.2f}%\")\n",
    "\n",
    "# Run federated learning with client-specific noise adaptation\n",
    "federated_learning(clients, global_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
