{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3de2ef7981c4a09a1b2d487dc11d946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68524a2bdae54441be5620ed39c02223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a6009deb764e49b902dd4fc1f2492d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97cdff28f5a144f78e9cf07f78c6fe44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500ab4a8c17840b98e410b2287b32f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5876e7a6754348eeba95dd18247a8579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/850 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3385b68e704b4e79b0b753a26cc7521c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc503acc065146208480ad9c77925c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2a93730d9e4fb2bce2280c13198b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20dfcfd0cf9c44f692f4cb009b313cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/AnnantJain/client1_federated_dataset_with_splits/commit/16bbb7e293c3eaca95ba17a8cb69c6e076d034dc', commit_message='Upload dataset', commit_description='', oid='16bbb7e293c3eaca95ba17a8cb69c6e076d034dc', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "import random\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "dataset = load_dataset(\"AnnantJain/client1_federated_dataset\")\n",
    "\n",
    "# Extract clean and noisy subsets\n",
    "clean_images = dataset['clean']\n",
    "noisy_images = dataset['noisy']\n",
    "\n",
    "# Total number of images and samples for test/train splits\n",
    "total_images = len(clean_images) + len(noisy_images)\n",
    "test_size = int(0.15 * total_images)  # 15% of total images = 450 in your case\n",
    "train_size = total_images - test_size\n",
    "\n",
    "# Number of clean and noisy images needed for the test set\n",
    "noisy_test_size = int(0.50 * test_size)  # 60% of test set should be noisy\n",
    "clean_test_size = test_size - noisy_test_size  # 40% of test set should be clean\n",
    "\n",
    "# Shuffle the data using Hugging Face's `shuffle()`\n",
    "clean_images = clean_images.shuffle(seed=42)\n",
    "noisy_images = noisy_images.shuffle(seed=42)\n",
    "\n",
    "# Split the clean and noisy images into train and test sets using `.select()`\n",
    "clean_test_split = clean_images.select(range(clean_test_size))\n",
    "noisy_test_split = noisy_images.select(range(noisy_test_size))\n",
    "clean_train_split = clean_images.select(range(clean_test_size, len(clean_images)))\n",
    "noisy_train_split = noisy_images.select(range(noisy_test_size, len(noisy_images)))\n",
    "\n",
    "# Combine train and test splits using `concatenate_datasets`\n",
    "train_images = concatenate_datasets([clean_train_split, noisy_train_split])\n",
    "test_images = concatenate_datasets([clean_test_split, noisy_test_split])\n",
    "\n",
    "# Create a new DatasetDict for train and test splits\n",
    "new_dataset = DatasetDict({\n",
    "    'train': train_images,\n",
    "    'test': test_images\n",
    "})\n",
    "\n",
    "# Save or return the new dataset (make sure you're logged in to Hugging Face Hub)\n",
    "new_dataset.push_to_hub(\"AnnantJain/client1_federated_dataset_with_splits\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec475c9e0fe146d9a8b4ab05f29e209a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d53a92faab343e591bd1eec46e7087d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1952 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d771489cf3440ffa472c44fae00b69c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81ae318ae8e46919b23b4364db1b702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd9da858bbea462ca2043e15ec3ee07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/344 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484ca09abe624cec8e6fb412eefbc908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/AnnantJain/client2_federated_dataset_with_splits/commit/27d07faf96e7e702889f7fdf3e695d41186518ac', commit_message='Upload dataset', commit_description='', oid='27d07faf96e7e702889f7fdf3e695d41186518ac', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "import random\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "dataset = load_dataset(\"AnnantJain/client2_federated_dataset\")\n",
    "\n",
    "# Extract clean and noisy subsets\n",
    "clean_images = dataset['clean']\n",
    "noisy_images = dataset['noisy']\n",
    "\n",
    "# Total number of images and samples for test/train splits\n",
    "total_images = len(clean_images) + len(noisy_images)\n",
    "test_size = int(0.15 * total_images)  # 15% of total images = 450 in your case\n",
    "train_size = total_images - test_size\n",
    "\n",
    "# Number of clean and noisy images needed for the test set\n",
    "noisy_test_size = int(0.60 * test_size)  # 60% of test set should be noisy\n",
    "clean_test_size = test_size - noisy_test_size  # 40% of test set should be clean\n",
    "\n",
    "# Shuffle the data using Hugging Face's `shuffle()`\n",
    "clean_images = clean_images.shuffle(seed=42)\n",
    "noisy_images = noisy_images.shuffle(seed=42)\n",
    "\n",
    "# Split the clean and noisy images into train and test sets using `.select()`\n",
    "clean_test_split = clean_images.select(range(clean_test_size))\n",
    "noisy_test_split = noisy_images.select(range(noisy_test_size))\n",
    "clean_train_split = clean_images.select(range(clean_test_size, len(clean_images)))\n",
    "noisy_train_split = noisy_images.select(range(noisy_test_size, len(noisy_images)))\n",
    "\n",
    "# Combine train and test splits using `concatenate_datasets`\n",
    "train_images = concatenate_datasets([clean_train_split, noisy_train_split])\n",
    "test_images = concatenate_datasets([clean_test_split, noisy_test_split])\n",
    "\n",
    "# Create a new DatasetDict for train and test splits\n",
    "new_dataset = DatasetDict({\n",
    "    'train': train_images,\n",
    "    'test': test_images\n",
    "})\n",
    "\n",
    "# Save or return the new dataset (make sure you're logged in to Hugging Face Hub)\n",
    "new_dataset.push_to_hub(\"AnnantJain/client2_federated_dataset_with_splits\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53b1ba4e6f6453993b476caa8b50f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c313db4c93450e96ad7b33ef7cc8cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3b9fae1ed541f48ccf8f5e2870bbda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/17 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e160f3b890a42d08aebfde1833d0f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a118a21f6eea498c9cf2f8676458d900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42023473f3a840df8b9d0b616bfd9a09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/AnnantJain/client3_federated_dataset_with_splits/commit/999ec2e27fde963130c036184575af44ae73c8c3', commit_message='Upload dataset', commit_description='', oid='999ec2e27fde963130c036184575af44ae73c8c3', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "import random\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "dataset = load_dataset(\"AnnantJain/client3_federated_dataset\")\n",
    "\n",
    "# Extract clean and noisy subsets\n",
    "clean_images = dataset['clean']\n",
    "noisy_images = dataset['noisy']\n",
    "\n",
    "# Total number of images and samples for test/train splits\n",
    "total_images = len(clean_images) + len(noisy_images)\n",
    "test_size = int(0.15 * total_images)  # 15% of total images = 450 in your case\n",
    "train_size = total_images - test_size\n",
    "\n",
    "# Number of clean and noisy images needed for the test set\n",
    "noisy_test_size = int(0.60 * test_size)  # 60% of test set should be noisy\n",
    "clean_test_size = test_size - noisy_test_size  # 40% of test set should be clean\n",
    "\n",
    "# Shuffle the data using Hugging Face's `shuffle()`\n",
    "clean_images = clean_images.shuffle(seed=42)\n",
    "noisy_images = noisy_images.shuffle(seed=42)\n",
    "\n",
    "# Split the clean and noisy images into train and test sets using `.select()`\n",
    "clean_test_split = clean_images.select(range(clean_test_size))\n",
    "noisy_test_split = noisy_images.select(range(noisy_test_size))\n",
    "clean_train_split = clean_images.select(range(clean_test_size, len(clean_images)))\n",
    "noisy_train_split = noisy_images.select(range(noisy_test_size, len(noisy_images)))\n",
    "\n",
    "# Combine train and test splits using `concatenate_datasets`\n",
    "train_images = concatenate_datasets([clean_train_split, noisy_train_split])\n",
    "test_images = concatenate_datasets([clean_test_split, noisy_test_split])\n",
    "\n",
    "# Create a new DatasetDict for train and test splits\n",
    "new_dataset = DatasetDict({\n",
    "    'train': train_images,\n",
    "    'test': test_images\n",
    "})\n",
    "\n",
    "# Save or return the new dataset (make sure you're logged in to Hugging Face Hub)\n",
    "new_dataset.push_to_hub(\"AnnantJain/client3_federated_dataset_with_splits\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13446269e96e48abafbc71a9d68495f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b04cd45e59481591db9e46e44728ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a243207a42546e5af7eb0941f648f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b48f39df309e42c3997e8de609c4df9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e63ac616a340a78ac172418cfd3bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31beaa11a8084828bf12cbec2a4c6722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/AnnantJain/client4_federated_dataset_with_splits/commit/cd8668e0e9903738f09d8eb9ddc6c229849c2c66', commit_message='Upload dataset', commit_description='', oid='cd8668e0e9903738f09d8eb9ddc6c229849c2c66', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "import random\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "dataset = load_dataset(\"AnnantJain/client4_federated_dataset\")\n",
    "\n",
    "# Extract clean and noisy subsets\n",
    "clean_images = dataset['clean']\n",
    "noisy_images = dataset['noisy']\n",
    "\n",
    "# Total number of images and samples for test/train splits\n",
    "total_images = len(clean_images) + len(noisy_images)\n",
    "test_size = int(0.15 * total_images)  # 15% of total images = 450 in your case\n",
    "train_size = total_images - test_size\n",
    "\n",
    "# Number of clean and noisy images needed for the test set\n",
    "noisy_test_size = int(0.60 * test_size)  # 60% of test set should be noisy\n",
    "clean_test_size = test_size - noisy_test_size  # 40% of test set should be clean\n",
    "\n",
    "# Shuffle the data using Hugging Face's `shuffle()`\n",
    "clean_images = clean_images.shuffle(seed=42)\n",
    "noisy_images = noisy_images.shuffle(seed=42)\n",
    "\n",
    "# Split the clean and noisy images into train and test sets using `.select()`\n",
    "clean_test_split = clean_images.select(range(clean_test_size))\n",
    "noisy_test_split = noisy_images.select(range(noisy_test_size))\n",
    "clean_train_split = clean_images.select(range(clean_test_size, len(clean_images)))\n",
    "noisy_train_split = noisy_images.select(range(noisy_test_size, len(noisy_images)))\n",
    "\n",
    "# Combine train and test splits using `concatenate_datasets`\n",
    "train_images = concatenate_datasets([clean_train_split, noisy_train_split])\n",
    "test_images = concatenate_datasets([clean_test_split, noisy_test_split])\n",
    "\n",
    "# Create a new DatasetDict for train and test splits\n",
    "new_dataset = DatasetDict({\n",
    "    'train': train_images,\n",
    "    'test': test_images\n",
    "})\n",
    "\n",
    "# Save or return the new dataset (make sure you're logged in to Hugging Face Hub)\n",
    "new_dataset.push_to_hub(\"AnnantJain/client4_federated_dataset_with_splits\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d03d5982a804d5a918b27bc6cf8dc77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b35767ad714b4e6c94ed9edbb66ecf73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1360 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae55a323ef84077bd9237307c996940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eef997503c14289b9002f50e73cbe76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1360 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf6ca8f16a4401ea16da381431c1a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05819b36a8543c19693c1a6858d9ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86bda16f980f4e418ab6f6517d7fec91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/480 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95faf71ec57b40a49c525bcbf1ee5fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/AnnantJain/client5_federated_dataset_with_splits/commit/190c244598492069c5a342eb26e767f85a602679', commit_message='Upload dataset', commit_description='', oid='190c244598492069c5a342eb26e767f85a602679', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "import random\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "dataset = load_dataset(\"AnnantJain/client5_federated_dataset\")\n",
    "\n",
    "# Extract clean and noisy subsets\n",
    "clean_images = dataset['clean']\n",
    "noisy_images = dataset['noisy']\n",
    "\n",
    "# Total number of images and samples for test/train splits\n",
    "total_images = len(clean_images) + len(noisy_images)\n",
    "test_size = int(0.15 * total_images)  # 15% of total images = 450 in your case\n",
    "train_size = total_images - test_size\n",
    "\n",
    "# Number of clean and noisy images needed for the test set\n",
    "noisy_test_size = int(0.60 * test_size)  # 60% of test set should be noisy\n",
    "clean_test_size = test_size - noisy_test_size  # 40% of test set should be clean\n",
    "\n",
    "# Shuffle the data using Hugging Face's `shuffle()`\n",
    "clean_images = clean_images.shuffle(seed=42)\n",
    "noisy_images = noisy_images.shuffle(seed=42)\n",
    "\n",
    "# Split the clean and noisy images into train and test sets using `.select()`\n",
    "clean_test_split = clean_images.select(range(clean_test_size))\n",
    "noisy_test_split = noisy_images.select(range(noisy_test_size))\n",
    "clean_train_split = clean_images.select(range(clean_test_size, len(clean_images)))\n",
    "noisy_train_split = noisy_images.select(range(noisy_test_size, len(noisy_images)))\n",
    "\n",
    "# Combine train and test splits using `concatenate_datasets`\n",
    "train_images = concatenate_datasets([clean_train_split, noisy_train_split])\n",
    "test_images = concatenate_datasets([clean_test_split, noisy_test_split])\n",
    "\n",
    "# Create a new DatasetDict for train and test splits\n",
    "new_dataset = DatasetDict({\n",
    "    'train': train_images,\n",
    "    'test': test_images\n",
    "})\n",
    "\n",
    "# Save or return the new dataset (make sure you're logged in to Hugging Face Hub)\n",
    "new_dataset.push_to_hub(\"AnnantJain/client5_federated_dataset_with_splits\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
